{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4ce286-4b5d-42ab-881c-162e0ebd0543",
   "metadata": {},
   "source": [
    "# Project Title:\n",
    "**AI Guardian: An AI-Powered Consumer Protection Assistant**\n",
    "\n",
    "## Project Overview:\n",
    "The AI Guardian project aims to develop an AI-powered assistant designed to help consumers navigate the digital marketplace safely and confidently. The assistant will focus on providing reliable information, detecting scams and misinformation, and offering support for consumer rights and redress mechanisms.\n",
    "\n",
    "## Key Features:\n",
    "1. **Trustworthy Information Source**:\n",
    "   - **Verified Answers**: Use natural language processing (NLP) to provide consumers with verified and citation-backed answers to their queries.\n",
    "   - **Bias Detection**: Implement algorithms to detect and mitigate regional or cultural biases in information provided.\n",
    "\n",
    "2. **Scam and Misinformation Detection**:\n",
    "   - **Real-time Scam Alerts**: Use machine learning to identify and alert users about potential scams in real-time, whether in emails, advertisements, or online shopping platforms.\n",
    "   - **Misinformation Filtering**: Analyze content from various sources and flag misinformation, providing users with accurate and verified alternatives.\n",
    "\n",
    "3. **Consumer Rights Education**:\n",
    "   - **Interactive Learning Modules**: Offer interactive and engaging modules to educate consumers about their rights, especially in relation to AI and digital services.\n",
    "   - **Guided Redress Mechanisms**: Provide step-by-step guidance on how consumers can seek redress for issues with AI technologies or digital services.\n",
    "\n",
    "4. **Personalized Recommendations and Alerts**:\n",
    "   - **Custom Alerts**: Based on user preferences and past behavior, the assistant can offer personalized alerts about new scams or critical updates in consumer protection laws.\n",
    "   - **Safe Shopping Recommendations**: Recommend safe and reliable online marketplaces and vendors based on AI analysis of consumer reviews and ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f1f0a-0a15-4d8d-b16a-27fea4593b8d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca5a0ddf-5c1c-4635-b3f4-97b784cc7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.memory import ChatMessageHistory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f2387-f84c-4aa5-b1f1-39ecb4b4b57b",
   "metadata": {},
   "source": [
    "## Load API KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4006b9b-626c-45e9-aab9-fca44afcb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa97c8-1d53-4ed0-8027-861ab3c0a43f",
   "metadata": {},
   "source": [
    "## Initializing Chat Model\n",
    "Let's initialize the chat model which will serve as the chatbot's brain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b418a86a-6d76-41be-ae1d-17b49bb0cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = 'llama3-8b-8192'\n",
    "\n",
    "chat = ChatGroq(temperature=0, groq_api_key=GROQ_API_KEY, model_name=Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ca7a6-ddb5-4d83-906f-5d882b44671d",
   "metadata": {},
   "source": [
    "## Invoking The Model\n",
    "If we invoke our chat model, the output is an AIMessage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77274448-8549-4154-8e09-79995ab95a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Large Language Models (LLMs) have revolutionized the field of natural language processing (NLP) by enabling applications such as language translation, text summarization, and chatbots. However, traditional LLMs often suffer from high latency, which can be a significant limitation in many applications. Low latency LLMs, on the other hand, offer several advantages that make them crucial for various use cases. Here are some reasons why low latency LLMs are important:\\n\\n1. **Real-time applications**: In applications like customer service chatbots, voice assistants, or real-time language translation, low latency is essential to provide a seamless user experience. Low latency LLMs enable faster response times, reducing the likelihood of user frustration and improving overall user satisfaction.\\n2. **Interactive applications**: Interactive applications like gaming, virtual reality, or augmented reality rely heavily on low latency LLMs to provide a smooth and immersive experience. Low latency LLMs can process user input quickly, reducing lag and improving overall responsiveness.\\n3. **Edge computing**: With the increasing adoption of edge computing, low latency LLMs can be deployed closer to the user, reducing latency and improving performance. This is particularly important in applications like autonomous vehicles, where low latency is critical for real-time decision-making.\\n4. **Improved user engagement**: Low latency LLMs can improve user engagement by providing faster response times, reducing the likelihood of user abandonment. This is particularly important in applications like online shopping, where a slow response can lead to a lost sale.\\n5. **Enhanced accuracy**: Low latency LLMs can improve the accuracy of NLP applications by reducing the likelihood of errors caused by delayed processing. This is particularly important in applications like medical diagnosis, where accuracy is critical.\\n6. **Scalability**: Low latency LLMs can be designed to scale more efficiently, allowing them to handle a larger volume of requests without compromising performance. This is particularly important in applications like social media, where high traffic volumes are common.\\n7. **Cost-effective**: Low latency LLMs can be more cost-effective than traditional LLMs, as they require less computational resources and can be deployed on edge devices or cloud infrastructure.\\n\\nIn summary, low latency LLMs are essential for various applications that require real-time processing, interactive experiences, and improved user engagement. By reducing latency, low latency LLMs can improve the overall user experience, enhance accuracy, and provide a competitive advantage in the market.', response_metadata={'token_usage': {'completion_time': 0.588, 'completion_tokens': 496, 'prompt_time': 0.009, 'prompt_tokens': 32, 'queue_time': None, 'total_time': 0.597, 'total_tokens': 528}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_dadc9d6142', 'finish_reason': 'stop', 'logprobs': None}, id='run-e69998b4-8ace-4cee-9407-496c07e9c100-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e51b6633-2326-4341-9319-99832ac481bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The translation of \"I love programming\" in French is:\\n\\nJ\\'adore le programmation.\\n\\nHere\\'s a breakdown of the translation:\\n\\n* \"I\" is translated to \"J\\'\" (the subject pronoun in French)\\n* \"love\" is translated to \"adore\" (which is a stronger form of \"aimer\", meaning \"to love\")\\n* \"programming\" is translated to \"le programmation\" (with the definite article \"le\" because \"programmation\" is a masculine noun)\\n\\nSo, \"J\\'adore le programmation\" is the correct translation of \"I love programming\" in French!', response_metadata={'token_usage': {'completion_time': 0.151, 'completion_tokens': 128, 'prompt_time': 0.006, 'prompt_tokens': 22, 'queue_time': None, 'total_time': 0.157, 'total_tokens': 150}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-bc112877-745b-4622-8f14-2318e5ac9b8f-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to French: I love programming.\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05fb277-6efc-43f2-b22b-b59a46db1094",
   "metadata": {},
   "source": [
    "## Model State\n",
    "The model on its own does not have any concept of state. For example, if you ask a followup question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a970fc3-02ae-4b7d-a7b2-ada03717e8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I apologize, but this conversation just started. I haven't said anything yet. I'm here to help answer your questions and provide information. What would you like to talk about?\", response_metadata={'token_usage': {'completion_time': 0.041, 'completion_tokens': 36, 'prompt_time': 0.005, 'prompt_tokens': 16, 'queue_time': None, 'total_time': 0.046, 'total_tokens': 52}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_dadc9d6142', 'finish_reason': 'stop', 'logprobs': None}, id='run-5dd65440-f1a6-4050-891f-a9882d487bc1-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke([HumanMessage(content=\"What did you just say?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d79d5c-5a8a-4fe4-8ef4-143c7cb63c04",
   "metadata": {},
   "source": [
    "We can see that it doesn't take the previous conversation turn into context, and cannot answer the question.\n",
    "\n",
    "To get around this, we need to pass the entire conversation history into the model. Let's see what happens when we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94df4cc3-0303-4206-80c1-51c24fe53bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I translated the sentence \"I love programming\" from English to French. The translation is:\\n\\nJ\\'adore la programmation.\\n\\nHere\\'s a breakdown of the translation:\\n\\n* \"I\" is translated to \"J\\'\" (note the accent on the \"J\" to indicate the contraction)\\n* \"love\" is translated to \"adore\" (which means \"to love\" or \"to adore\")\\n* \"programming\" is translated to \"la programmation\" (using the definite article \"la\" to indicate that it\\'s a singular noun)\\n\\nSo, the entire sentence \"J\\'adore la programmation\" means \"I love programming\" in French!', response_metadata={'token_usage': {'completion_time': 0.158, 'completion_tokens': 133, 'prompt_time': 0.014, 'prompt_tokens': 46, 'queue_time': None, 'total_time': 0.17200000000000001, 'total_tokens': 179}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a6db999-a01c-4acc-8c93-b0eedfb9332c-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to French: I love programming.\"\n",
    "        ),\n",
    "        AIMessage(content=\"J'adore la programmation.\"),\n",
    "        HumanMessage(content=\"What did you just say?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ff1cf-be20-4cb5-9e9a-4e0bae4d5f94",
   "metadata": {},
   "source": [
    "And now we can see that we get a good response!\n",
    "\n",
    "This is the basic idea underpinning a chatbot's ability to interact conversationally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b4c81-c1e8-40fb-9bcf-b8c620f3be75",
   "metadata": {},
   "source": [
    "## Prompt templates\n",
    "Let's define a prompt template to make formatting a bit easier. We can create a chain by piping it into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bed58da-979e-45b0-9ea4-4fc2109f6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2ff7af-6f3d-4818-a9b1-de6fd106f806",
   "metadata": {},
   "source": [
    "The MessagesPlaceholder above inserts chat messages passed into the chain's input as chat_history directly into the prompt. Then, we can invoke the chain like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "010664b0-4e62-40ce-9d09-756e839df8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I translated the sentence \"I love programming\" from English to French. The translation is: \"J\\'adore la programmation\".', response_metadata={'token_usage': {'completion_time': 0.031, 'completion_tokens': 27, 'prompt_time': 0.017, 'prompt_tokens': 67, 'queue_time': None, 'total_time': 0.048, 'total_tokens': 94}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-6692dba9-baff-469c-8389-916e3d1ff07d-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Translate this sentence from English to French: I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"J'adore la programmation.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1451537b-8e6e-4a9a-ae3b-93f1da177222",
   "metadata": {},
   "source": [
    "## Message history\n",
    "As a shortcut for managing the chat history, we can use a MessageHistory class, which is responsible for saving and loading chat messages. There are many built-in message history integrations that persist messages to a variety of databases, but for this quickstart we'll use a in-memory, demo message history called ChatMessageHistory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25239e2e-e1ad-4f9b-ad02-2d36ee225715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!'), AIMessage(content='whats up?')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"hi!\")\n",
    "\n",
    "demo_ephemeral_chat_history.add_ai_message(\"whats up?\")\n",
    "\n",
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e1e0c-0330-48a5-8398-210fae25d94c",
   "metadata": {},
   "source": [
    "Once we do that, we can pass the stored messages directly into our chain as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19e9657c-73a1-4a42-a46b-c44dfa35d1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The translation of the sentence \"I love programming\" from English to French is:\\n\\nJ\\'adore le programmation.\\n\\nLet me know if you need any further assistance!', response_metadata={'token_usage': {'completion_time': 0.039, 'completion_tokens': 34, 'prompt_time': 0.014, 'prompt_tokens': 59, 'queue_time': None, 'total_time': 0.053, 'total_tokens': 93}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_af05557ca2', 'finish_reason': 'stop', 'logprobs': None}, id='run-fc9de046-c4dd-4a17-9396-d8287b9f365b-0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.add_user_message(\n",
    "    \"Translate this sentence from English to French: I love programming.\"\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30221b1b-3869-4c04-a19f-3bad5f6bc605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I translated the sentence \"I love programming\" from English to French. The French translation is \"J\\'adore le programmation\".', response_metadata={'token_usage': {'completion_time': 0.031, 'completion_tokens': 27, 'prompt_time': 0.027, 'prompt_tokens': 109, 'queue_time': None, 'total_time': 0.057999999999999996, 'total_tokens': 136}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_873a560973', 'finish_reason': 'stop', 'logprobs': None}, id='run-1ea1376b-8874-43b4-b6e4-d5011ffef7f1-0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.add_ai_message(response)\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"What did you just say?\")\n",
    "\n",
    "chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afdd4fb-5119-4d12-b976-1e53610e72b7",
   "metadata": {},
   "source": [
    "**And now we have a basic chatbot!**\n",
    "\n",
    "```While this chain can serve as a useful chatbot on its own with just the model's internal knowledge, it's often useful to introduce some form of retrieval-augmented generation, or RAG for short, over domain-specific knowledge to make our chatbot more focused. We'll cover this next.```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed51f1-3b7e-414a-a95c-7d3a18faff97",
   "metadata": {},
   "source": [
    "## Retrievers\n",
    "We can set up and use a Retriever to pull domain-specific knowledge for our chatbot. To show this, let's expand the simple chatbot we created above to be able to answer questions about LangSmith.\n",
    "\n",
    "We'll use the **Consumer Day and related website** as source material and store it in a vectorstore for later retrieval. Note that this example will gloss over some of the specifics around parsing and storing a data source.\n",
    "\n",
    "To go Deeper - ```https://python.langchain.com/v0.1/docs/use_cases/question_answering/```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d97d00d-11c1-43cc-af07-62ea054479c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://unctad.org/system/files/official-document/ditccplpmisc2016d1_en.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e52551d-10a2-4315-83f1-1cb7fb690c19",
   "metadata": {},
   "source": [
    "Next, we split it into smaller chunks that the LLM's context window can handle and store it in a vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72235d3b-9d1a-4385-b84d-67350121adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b8cc83-1c0b-4e24-a8be-6ead44f956ed",
   "metadata": {},
   "source": [
    "Then we embed and store those chunks in a vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47194b38-88ba-4a83-85dd-a080baba6127",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OllamaEmbeddings(model=\"llama3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26967f09-8c7c-4205-b74b-0d1111c328ba",
   "metadata": {},
   "source": [
    "And finally, let's create a retriever from our initialized vectorstore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de43bf-d9e3-4345-b813-97b98cc23daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k is the number of chunks to retrieve\n",
    "retriever = vectorstore.as_retriever(k=4)\n",
    "\n",
    "docs = retriever.invoke(\"how can langsmith help with testing?\")\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdccbf77-b3a8-4a28-a918-e2d3d86ec302",
   "metadata": {},
   "source": [
    "We can see that invoking the retriever above results in some parts of the UN guiedins for Consumer Protection document that contain information about testing that our chatbot can use as context when answering questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3664b-7972-4769-bd96-81d4718797fa",
   "metadata": {},
   "source": [
    "## Handling documents\n",
    "Let's modify our previous prompt to accept documents as context. We'll use a create_stuff_documents_chain helper function to \"stuff\" all of the input documents into the prompt, which also conveniently handles formatting. We use the ChatPromptTemplate.from_messages method to format the message input we want to pass to the model, including a MessagesPlaceholder where chat history messages will be directly injected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98017020-5d73-4ae6-962f-bc1db50bbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user's questions based on the below context:\\n\\n{context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43fe6ba-c060-4736-bd29-4ed0fa04dd9e",
   "metadata": {},
   "source": [
    "We can invoke this document_chain with the raw documents we retrieved above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1ae71-3ce8-4382-b402-20e9ddd0761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"how can langsmith help with testing?\")\n",
    "\n",
    "document_chain.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history.messages,\n",
    "        \"context\": docs,\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

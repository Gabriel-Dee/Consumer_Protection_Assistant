{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4ce286-4b5d-42ab-881c-162e0ebd0543",
   "metadata": {},
   "source": [
    "# Project Title:\n",
    "**AI Guardian: An AI-Powered Consumer Protection Assistant**\n",
    "\n",
    "## Project Overview:\n",
    "The AI Guardian project aims to develop an AI-powered assistant designed to help consumers navigate the digital marketplace safely and confidently. The assistant will focus on providing reliable information, detecting scams and misinformation, and offering support for consumer rights and redress mechanisms.\n",
    "\n",
    "## Key Features:\n",
    "1. **Trustworthy Information Source**:\n",
    "   - **Verified Answers**: Use natural language processing (NLP) to provide consumers with verified and citation-backed answers to their queries.\n",
    "   - **Bias Detection**: Implement algorithms to detect and mitigate regional or cultural biases in information provided.\n",
    "\n",
    "2. **Scam and Misinformation Detection**:\n",
    "   - **Real-time Scam Alerts**: Use machine learning to identify and alert users about potential scams in real-time, whether in emails, advertisements, or online shopping platforms.\n",
    "   - **Misinformation Filtering**: Analyze content from various sources and flag misinformation, providing users with accurate and verified alternatives.\n",
    "\n",
    "3. **Consumer Rights Education**:\n",
    "   - **Interactive Learning Modules**: Offer interactive and engaging modules to educate consumers about their rights, especially in relation to AI and digital services.\n",
    "   - **Guided Redress Mechanisms**: Provide step-by-step guidance on how consumers can seek redress for issues with AI technologies or digital services.\n",
    "\n",
    "4. **Personalized Recommendations and Alerts**:\n",
    "   - **Custom Alerts**: Based on user preferences and past behavior, the assistant can offer personalized alerts about new scams or critical updates in consumer protection laws.\n",
    "   - **Safe Shopping Recommendations**: Recommend safe and reliable online marketplaces and vendors based on AI analysis of consumer reviews and ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f1f0a-0a15-4d8d-b16a-27fea4593b8d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca5a0ddf-5c1c-4635-b3f4-97b784cc7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import dotenv\n",
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from typing import Dict\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f2387-f84c-4aa5-b1f1-39ecb4b4b57b",
   "metadata": {},
   "source": [
    "## Load API KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4006b9b-626c-45e9-aab9-fca44afcb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa97c8-1d53-4ed0-8027-861ab3c0a43f",
   "metadata": {},
   "source": [
    "## Initializing Chat Model\n",
    "Let's initialize the chat model which will serve as the chatbot's brain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b418a86a-6d76-41be-ae1d-17b49bb0cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "chat = ChatOpenAI(model=Model, temperature=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ca7a6-ddb5-4d83-906f-5d882b44671d",
   "metadata": {},
   "source": [
    "## Message passing\n",
    "The simplest form of memory is simply passing chat history messages into a chain. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba43c22-6835-42bc-bcba-f9b7e6ddaf18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I said \"J\\'adore la programmation,\" which means \"I love programming\" in French.', response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 61, 'total_tokens': 82}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d965bb4c-6e8f-4552-a59c-d742e5a0342c-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Translate this sentence from English to French: I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"J'adore la programmation.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fbc40d-808e-4dca-8bd8-fffcd81d3635",
   "metadata": {},
   "source": [
    "We can see that by passing the previous conversation into a chain, it can use it as context to answer questions. This is the basic concept underpinning chatbot memory - the rest of the guide will demonstrate convenient techniques for passing or reformatting messages.\n",
    "\n",
    "## Chat history\n",
    "It's perfectly fine to store and pass messages directly as an array, but we can use LangChain's built-in message history class to store and load messages as well. Instances of this class are responsible for storing and loading chat messages from persistent storage. LangChain integrates with many providers - you can see a list of integrations here - but for this demo we will use an ephemeral demo class.\n",
    "\n",
    "Here's an example of the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11227750-6182-46f9-8034-0738cf62da20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Translate this sentence from English to French: I love programming.'),\n",
       " AIMessage(content=\"J'adore la programmation.\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\n",
    "    \"Translate this sentence from English to French: I love programming.\"\n",
    ")\n",
    "\n",
    "demo_ephemeral_chat_history.add_ai_message(\"J'adore la programmation.\")\n",
    "\n",
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6073d2f-d869-4ad0-9708-b0f8f5479bf0",
   "metadata": {},
   "source": [
    "We can use it directly to store conversation turns for our chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d54fa7e3-12c7-49f4-b673-80081eb810df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You just asked me to translate the sentence \"I love programming\" from English to French.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 74, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-de09fa13-c193-4edd-a5c4-5c52f7445aec-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "input1 = \"Translate this sentence from English to French: I love programming.\"\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(input1)\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history.messages,\n",
    "    }\n",
    ")\n",
    "\n",
    "demo_ephemeral_chat_history.add_ai_message(response)\n",
    "\n",
    "input2 = \"What did I just ask you?\"\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(input2)\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history.messages,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdae045-78da-46a0-b63c-26799890f3b4",
   "metadata": {},
   "source": [
    "## Automatic history management\n",
    "The previous examples pass messages to the chain explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also includes an wrapper for LCEL chains that can handle this process automatically called RunnableWithMessageHistory.\n",
    "\n",
    "To show how it works, let's slightly modify the above prompt to take a final input variable that populates a HumanMessage template after the chat history. This means that we will expect a chat_history parameter that contains all messages BEFORE the current messages instead of all messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e19076a1-1967-4e9e-8d58-538e5e49e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4739120c-d3ed-4165-901b-18ca9a37214b",
   "metadata": {},
   "source": [
    "We'll pass the latest input to the conversation here and let the RunnableWithMessageHistory class wrap our chain and do the work of appending that input variable to the chat history.\n",
    "\n",
    "Next, let's declare our wrapped chain:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe0bec9-84c1-4e77-b1e7-76725569edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history_for_chain = ChatMessageHistory()\n",
    "\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: demo_ephemeral_chat_history_for_chain,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128237a5-27fa-4557-b7a0-cee7bbef861d",
   "metadata": {},
   "source": [
    "This class takes a few parameters in addition to the chain that we want to wrap:\n",
    "\n",
    "A factory function that returns a message history for a given session id. This allows your chain to handle multiple users at once by loading different messages for different conversations.\n",
    "An input_messages_key that specifies which part of the input should be tracked and stored in the chat history. In this example, we want to track the string passed in as input.\n",
    "A history_messages_key that specifies what the previous messages should be injected into the prompt as. Our prompt has a MessagesPlaceholder named chat_history, so we specify this property to match.\n",
    "(For chains with multiple outputs) an output_messages_key which specifies which output to store as history. This is the inverse of input_messages_key.\n",
    "We can invoke this new chain as normal, with an additional configurable field that specifies the particular session_id to pass to the factory function. This is unused for the demo, but in real-world chains, you'll want to return a chat history corresponding to the passed session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e21e15d-8f05-480e-9f3f-e41ae6634db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The translation of \"I love programming\" in French is \"J\\'adore la programmation.\"', response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 39, 'total_tokens': 59}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c442431a-8216-4221-b103-fc6a94181a63-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_message_history.invoke(\n",
    "    {\"input\": \"Translate this sentence from English to French: I love programming.\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa5a4d5c-f441-4604-b1b7-bcd6507c9baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You just asked me to translate the sentence \"I love programming\" from English to French.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 74, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b940ac30-a83a-4f0d-bf26-02bd0fe08369-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_message_history.invoke(\n",
    "    {\"input\": \"What did I just ask you?\"}, {\"configurable\": {\"session_id\": \"unused\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357cb2a6-f8c7-4ccc-8e72-12767667c491",
   "metadata": {},
   "source": [
    "## Modifying chat history\n",
    "Modifying stored chat messages can help your chatbot handle a variety of situations. Here are some examples:\n",
    "\n",
    "### Trimming messages\n",
    "LLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is to only load and store the most recent n messages. Let's use an example history with some preloaded messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f64fbac2-d9c5-4b55-b5fb-bf9f7e925698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
       " AIMessage(content='Hello!'),\n",
       " HumanMessage(content='How are you today?'),\n",
       " AIMessage(content='Fine thanks!')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"Hey there! I'm Nemo.\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Hello!\")\n",
    "demo_ephemeral_chat_history.add_user_message(\"How are you today?\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Fine thanks!\")\n",
    "\n",
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805152e5-5c27-4e61-b1e1-d8bb85c04130",
   "metadata": {},
   "source": [
    "Let's use this message history with the RunnableWithMessageHistory chain we declared above:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65d1e2dc-a304-403d-b5f1-32bc470d8164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Nemo.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 66, 'total_tokens': 72}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5761849f-a925-4be3-b5a2-efd6cbde3652-0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: demo_ephemeral_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "chain_with_message_history.invoke(\n",
    "    {\"input\": \"What's my name?\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd9e86a-bb0e-4797-aaf8-de5cce34b68f",
   "metadata": {},
   "source": [
    "We can see the chain remembers the preloaded name.\n",
    "\n",
    "But let's say we have a very small context window, and we want to trim the number of messages passed to the chain to only the 2 most recent ones. We can use the clear method to remove messages and re-add them to the history. We don't have to, but let's put this method at the front of our chain to ensure it's always called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "642751fd-918d-4914-9d3f-913da2333bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def trim_messages(chain_input):\n",
    "    stored_messages = demo_ephemeral_chat_history.messages\n",
    "    if len(stored_messages) <= 2:\n",
    "        return False\n",
    "\n",
    "    demo_ephemeral_chat_history.clear()\n",
    "\n",
    "    for message in stored_messages[-2:]:\n",
    "        demo_ephemeral_chat_history.add_message(message)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "chain_with_trimming = (\n",
    "    RunnablePassthrough.assign(messages_trimmed=trim_messages)\n",
    "    | chain_with_message_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a6cf1da-89c2-486c-80b5-6fcfcf8b6452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"P. Sherman's address is 42 Wallaby Way, Sydney.\", response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 53, 'total_tokens': 67}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f11b215f-8f2f-4a2f-8a3e-6d18b86a5472-0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_trimming.invoke(\n",
    "    {\"input\": \"Where does P. Sherman live?\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76cd5a66-5d0d-453c-8fa3-db2131db1048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"What's my name?\"),\n",
       " AIMessage(content='Your name is Nemo.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 66, 'total_tokens': 72}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5761849f-a925-4be3-b5a2-efd6cbde3652-0'),\n",
       " HumanMessage(content='Where does P. Sherman live?'),\n",
       " AIMessage(content=\"P. Sherman's address is 42 Wallaby Way, Sydney.\", response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 53, 'total_tokens': 67}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f11b215f-8f2f-4a2f-8a3e-6d18b86a5472-0')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6280cfc9-ecde-48e8-b92e-ea8a16c708de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, I don't have access to your personal information. Therefore, I don't know your name.\", response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 61, 'total_tokens': 84}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d184170c-1bb2-4d21-a96c-cedfd7fd7c06-0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_trimming.invoke(\n",
    "    {\"input\": \"What is my name?\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ed032ed-06af-4a0d-b000-805e318867b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Where does P. Sherman live?'),\n",
       " AIMessage(content=\"P. Sherman's address is 42 Wallaby Way, Sydney.\", response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 53, 'total_tokens': 67}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f11b215f-8f2f-4a2f-8a3e-6d18b86a5472-0'),\n",
       " HumanMessage(content='What is my name?'),\n",
       " AIMessage(content=\"I'm sorry, I don't have access to your personal information. Therefore, I don't know your name.\", response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 61, 'total_tokens': 84}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d184170c-1bb2-4d21-a96c-cedfd7fd7c06-0')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1477430-14de-421a-a8ae-598f3ff1e49a",
   "metadata": {},
   "source": [
    "## Summary memory\n",
    "We can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our chain. Let's recreate our chat history and chatbot chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a78019b-9135-4b19-8d4b-68b44bf73eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
       " AIMessage(content='Hello!'),\n",
       " HumanMessage(content='How are you today?'),\n",
       " AIMessage(content='Fine thanks!')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"Hey there! I'm Nemo.\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Hello!\")\n",
    "demo_ephemeral_chat_history.add_user_message(\"How are you today?\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Fine thanks!\")\n",
    "\n",
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfef8bf-babe-4359-b0ac-d8581fa3795e",
   "metadata": {},
   "source": [
    "We'll slightly modify the prompt to make the LLM aware that will receive a condensed summary instead of a chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cf4c75e-ca46-419a-804e-8c436ebd4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability. The provided chat history includes facts about the user you are speaking with.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: demo_ephemeral_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7179b2-1cac-4d32-bcb7-43ee5208fcb4",
   "metadata": {},
   "source": [
    "And now, let's create a function that will distill previous interactions into a summary. We can add this one to the front of the chain too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e59e227a-7d34-4e86-a757-5a2393df5651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_messages(chain_input):\n",
    "    stored_messages = demo_ephemeral_chat_history.messages\n",
    "    if len(stored_messages) == 0:\n",
    "        return False\n",
    "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Distill the above chat messages into a single summary message. Include as many specific details as you can.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    summarization_chain = summarization_prompt | chat\n",
    "\n",
    "    summary_message = summarization_chain.invoke({\"chat_history\": stored_messages})\n",
    "\n",
    "    demo_ephemeral_chat_history.clear()\n",
    "\n",
    "    demo_ephemeral_chat_history.add_message(summary_message)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "chain_with_summarization = (\n",
    "    RunnablePassthrough.assign(messages_summarized=summarize_messages)\n",
    "    | chain_with_message_history\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539049b8-17e3-4efa-9040-d770b6883677",
   "metadata": {},
   "source": [
    "Let's see if it remembers the name we gave it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c4f6099-6984-4a4e-943d-6bcf899741f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You introduced yourself as Nemo.', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 97, 'total_tokens': 104}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-757628ad-a63a-4650-b7db-19f0fcfb5b8a-0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_summarization.invoke(\n",
    "    {\"input\": \"What did I say my name was?\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "034d72c2-3bf6-40bc-94b0-73b0dd0b7874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='The conversation is between Nemo and an unnamed individual. Nemo introduces himself and the other person responds with a greeting. The other person asks Nemo how he is doing, and Nemo responds that he is fine.', response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 62, 'total_tokens': 106}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bd5cb3d7-a413-4615-8d4d-23d6debd38fa-0'),\n",
       " HumanMessage(content='What did I say my name was?'),\n",
       " AIMessage(content='You introduced yourself as Nemo.', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 97, 'total_tokens': 104}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-757628ad-a63a-4650-b7db-19f0fcfb5b8a-0')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bce4b2-d25e-4dc4-b0b5-1a10575b24b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
